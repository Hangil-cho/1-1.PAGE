1. Q. LLM(Large Lauguage Model)의 역사에 대해 알려줘
A. LLM(Large Language Model)의 역사는 다음과 같습니다.
1960년대: 초기의 언어 모델은 단순한 형태로, 이전 몇 개의 단어를 기반으로 문장의 다음 단어를 예측하는 방식으로 작동했습니다.
1980년대: 언어 모델링 분야에서 발전이 이루어졌으며, 언어 모델이 보다 복잡한 형태로 발전하기 시작했습니다. 이 시기에는 언어 모델이 문장의 의미를 이해하고, 문장의 맥락을 파악하는 기능이 추가되었습니다.
2010년대: 딥러닝 기술이 발전하면서, LLM의 학습에 딥러닝 알고리즘이 적용되기 시작했습니다. 딥러닝 알고리즘을 적용함으로써 LLM의 학습에 필요한 데이터의 양이 크게 감소하였으며, LLM의 성능이 크게 향상되었습니다.
2017년: 구글에서 발표한 Transformer 모델은 순차 데이터를 학습할 수 있는 신경망으로 LLM이 대두하는 계기가 되었습니다.
2018년: 구글에서 발표한 BERT(Pre-training of Deep Bidirectional Encoder Representations from Transformers)는 NLP 벤치마크에서 신기록을 세우게 됩니다.
2020년: OpenAI에서 GPT-3(Generative Pre-trained Transformer 3)를 발표하면서 LLM의 성능이 크게 향상되었습니다. GPT-3는 이전의 LLM과는 달리 1,750억 개의 파라미터를 가지고 있으며, 이를 통해 높은 수준의 자연어 처리 작업을 수행할 수 있습니다.
위와 같이 LLM의 역사는 언어 모델의 발전과 함께 지속적으로 발전해왔습니다.

source > https://ongamedev.tistory.com/530
